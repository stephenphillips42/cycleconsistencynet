\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

% Include other packages here, before hyperref.
\usepackage[outdir=./]{epstopdf}

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true,bookmarks=false]{hyperref}

% \cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

%% Custom control sequences
\newcommand{\bR}{\mathbb{R}}
\newcommand{\mat}[1]{\mathbf{#1}}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

% Pages are numbered in submission mode, and unnumbered in camera-ready
%\ifcvprfinal\pagestyle{empty}\fi
\setcounter{page}{4321}
\begin{document}

%%%%%%%%% TITLE
\title{Learning Multi-Image Matching using Cycle Consistency}

\author{Stephen Phillips, Kostas Daniilidis \\
GRASP Labratory, University of Pennsylvania\\
{\tt\small \{stephi, kostas\}@seas.upenn.edu}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
% \and
% Second Author\\
% Institution2\\
% First line of institution2 address\\
% {\tt\small secondauthor@i2.org}
}

\maketitle
%\thispagestyle{empty}

% Papers, excluding the references section,
% must be no longer than eight pages in length. The references section

%%%%%%%%% ABSTRACT
\begin{abstract}
   % Put abstract here
   % OK I hate this start but we will work on it.
   In this work we present a learning technique for refining multi-image matches in an unsupervised fashion.
   We formulate the multi-image matching problem as a graph embedding problem then use recent work in Graph Neural Networks to learn the appropriate embedding function.
   Since ground truth correspondence is difficult or expensive to come by in many real world datasets, we use cycle consistency as our loss function.
   Additional losses can be added if more information is available in the training set for better test performance.
   To the best of our knowledge, no other works have used learning for multi-image feature matching.
   Our experiments show that our method is competitive with other optimization based approaches.
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}

% Insert Introduction here
TODO




\begin{figure}[t]
\begin{center}
  \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
  % \includegraphics[width=0.8\linewidth]{figures/experiments.eps}
\end{center}
   \caption{Example of caption.  It is set in Roman so that mathematics
   (always set in Roman: $B \sin A = A \sin B$) may be included without an
   ugly clash.}
\label{fig:long}
\label{fig:onecol}
\end{figure}


%-------------------------------------------------------------------------
\section{Related Work}

TODO

% When placing figures in \LaTeX, it's almost always best to use
% \verb+\includegraphics+, and to specify the  figure width as a multiple of
% the line width as in the example below
% {\small\begin{verbatim}
%    \usepackage[dvips]{graphicx} ...
%    \includegraphics[width=0.8\linewidth]
%                    {myfile.eps}
% \end{verbatim}
% }


%------------------------------------------------------------------------
\section{Method}
Our goal is to find correspondences between multiple (more than two) images.
There are many techniques for finding such matchins in a pairwise setting, such as graph matching \cite{suh2015subgraph, hu2016distributable} or even learning techniques \cite{yi2018learning}. However, when only doing pairwise matching, errors can accumulate between the images. Our goal is to take a set of pairwise matches and to correct the accumulated pairwise matches.

\subsection{Multi-Image Matching}
In this section we first describe the multi-image matching problem and cycle consistency.
We are given a set of $n$ images $\mathcal{I} = \left\{ I_1, I_2, \ldots, I_n \right\}$.
Associated with each image $I_i$ is a set of $m_i$ features $\mat{Z}_i = \left\{ z_{i1}, z_{i2}, z_{i3}, \ldots, z_{i m_i} \right\}$.
Each feature corresponding to interesting points in the scene from the vantage point of that image.
We also assume that some pairwise matching algorithm has given us a set of matches between the images, represented as a partial permutation matrix, $\mat{M}_{ij} \in \bR^{m_i \times m_j}$, where $(\mat{M}_{ij})_{kl} = 1$ if the features match, and 0 otherwise.
Naturally, the matches are symmetric so $\mat{M}_{ij} = \mat{M}_{ji}$.
As is typical in the multi-image matching problem, we do not assume anything about how these pairwise matches were computed. 

However, as the $\mat{M}_{ij}$ are computed pairwise, no global concistency is guaranteed among them if there are errors. 
Thus we need a way to determine the global consistency of these matches.
This is where cycle consitency comes in.

\subsection{Cycle Consistency}
If the pairwise matches are globally consistent, then we can say that, for all $i, j, k$:
\begin{equation}
\mat{M}_{ij} = \mat{M}_{ik} \mat{M}_{kj}
\label{eq:cycconsist1}
\end{equation}
In other words, the matches between two images stays the same no matter what path is taken to get there. 
This constraint is known as \textit{cycle concistency}, and has been used in a number of recent works to optimze for global consistency \cite{zhou2015multi, wang2017multi, leonardos2016distributed}.
Stated in this form, there are $O(n^3)$ cycle consistency constraints to check.
A more elegant way to represent cycle consistency is to first create a `universe' of features that all images match to.
Then one can match each set of features $X_i$ to the universe, denoted $\mat{X}_i$.
Then the cycle consistency becomes:
\begin{equation}
\mat{M}_{ij} = \mat{X}_{i}\mat{X}_{j}^\top
\label{eq:cycconsist2}
\end{equation}
This can be more concisely described using the match matrices $\mat{M}$ and $\mat{X}$:
\begin{equation}
\mat{M} = \begin{pmatrix}
\mat{M}_{11} & \mat{M}_{12} & \hdots & \mat{M}_{1n} \\
\mat{M}_{21} & \mat{M}_{22} & \hdots & \mat{M}_{2n} \\
   \vdots    &     \vdots   & \ddots &    \vdots    \\
\mat{M}_{n1} & \mat{M}_{n2} & \hdots & \mat{M}_{nn}
\end{pmatrix}, \;
\mat{X} = \begin{pmatrix}
\mat{X}_{1}  \\
\mat{X}_{2}  \\
   \vdots     \\
\mat{X}_{n} 
\end{pmatrix}
\label{eq:matchmat}
\end{equation}
Now we define cycle consistency as:
\begin{equation}
\mat{M} = \mat{X} \mat{X}^\top
\label{eq:cycconsist3}
\end{equation}
In practice of course, $M$ has errors, and in fact the $\mat{M}_{ij}$ are typically not given as permutations but as matrices of similarity scores.
We denote this noisy version of the matches $\widetilde{\mat{M}}$.
Thus we can treat this as a low rank factorization problem on $\widetilde{\mat{M}}$, with permutation (or soft permutation) constraints on $\mat{X}$, as done in \cite{zhou2015multi, leonardos2016distributed, pachauri2013solving}:
\begin{align}
\argmin_{\mat{X}} \mathcal{L}(\mat{X})
=&\; \mathcal{D}\left(\widetilde{\mat{M}}, \mat{X}\mat{X}^\top\right) + \mathcal{R}(\mat{X}) \nonumber \\
\mathrm{s.t.} &\; \mat{X} \in \mathcal{X} \label{eq:genericloss}
\end{align}
Here $\mathcal{D}$ denotes some distance function and $\mathcal{R}$ denotes a regularizer on $\mat{X}$.

In previous work, the optimization is done directly.
Here we use learning to adapt to the statistics of the noise of the matching matrices.
Using Equation \ref{eq:genericloss} We have enough for a loss function to train a learning algorithm.
However, our input doesn't match typical image or sequence format most modern learning algorithms rely on.
Thus we need a different type of architecture to be able to perform learning on this task.

\subsection{Graph Neural Networks}
Now we show how to reformulate the problem in terms of graphs, and show how Graph Neural Networks can be applied to solve it.
To see how this is equivalent to a graph problem, we can simply view the $\widetilde{\mat{M}}$ as the adjacency matrix of a graph.
They cycle consistecy constraint is equivalent to there being $k$ disjoint subgraphs, with $k$ being the size of your `universe' of features. 
Ideally we would want to output a refined graph with the correct disjoint subgraphs corresponding to the ground truth features.
One could optimize for this, however, as we wish to use a learning algorithm, outputting a full graph would be difficult.
Instead we learn an embedding, equivalent to $\mat{X}$ in the previous section.

Graph Neural Networks (Graph NNs) have gotten more attention recently in the Machine Learning community \cite{bronstein2017geometric, bruna2013spectral, defferrard2016convolutional, kipf2016semi, scarselli2009graph, gama2018mimo, gama2018convolutional}.
Graph NNs have a long history and there are many varitions of it. 
However, we need one that can handle a different graphs as input and outputs an embedding on each node.
Here we follow the formulation from \cite{kipf2016semi} due to its simplicity, however there are many alternate architectures or formulations one could use in principle.
For this formulation, it is assumed we have some initial embedding on the graph $\mat{X}_0$ and some function of the adjacency matrix $A$ and degree matrix $D$, $L = (D+I)^{-1/2}(A+I)(D+I)^{-1/2}$.
The use of $L$ is more for convience, what is most important is that it is a function of the adjacency matrix.
Thus our update state is from the graph embedding on layer $l$ to layer $l + 1$:
\begin{equation*}
\mat{X}_{l+1} = \sigma\left( L X_l W_l \right)
\label{eq:gnnupdate}
\end{equation*}
This can be viewed as a linear combination of 


%------------------------------------------------------------------------
\section{Experiments}

\begin{figure*}
\begin{center}
  % \fbox{\rule{0pt}{2in} \rule{.9\linewidth}{0pt}}
  \includegraphics[width=0.8\linewidth]{figures/experiments01-eps-converted-to.pdf}
  \end{center}
     \caption{Example of a short caption, which should be centered.}
  \label{fig:short}
\end{figure*}

TODO
% \begin{table}
% \begin{center}
% \begin{tabular}{|l|c|}
% \hline
% Method & Frobnability \\
% \hline\hline
% Theirs & Frumpy \\
% Yours & Frobbly \\
% Ours & Makes one's heart Frob\\
% \hline
% \end{tabular}
% \end{center}
% \caption{Results.   Ours is better.}
% \end{table}


%------------------------------------------------------------------------
\section{Conclusion}

You must include your signed IEEE copyright release form when you submit
your finished paper. We MUST have this form before your paper can be
published in the proceedings.

Please direct any questions to the production editor in charge of these
proceedings at the IEEE Computer Society Press: Phone (714) 821-8380, or
Fax (714) 761-1784.

{\small
\bibliographystyle{ieee}
\bibliography{mybib}
}

\end{document}
