\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{All Graphs Lead to Rome: Rebuttal}  % **** Enter the paper title here

\maketitle
\thispagestyle{empty}


%%%%%%%%% BODY TEXT - ENTER YOUR RESPONSE BELOW
% TODO: Don't worry about space
% TODO: Do more bullet point style
% TODO: Thank all the reviewers at the start, skip sentences later
\section{Reviewer \#1}
Thank you for your comments and feedback. You
mention the benefits of using Graph Neural Networks are not clear. One
of the primary benefits is the ability to integrate the matching in an
end-to-end learning pipeline, which could for instance train on semantic
features. Adding in additional side-losses can assist in training,
even if such information is not available at testing time. While not
explored in this work, GNNs have great potential in fast parallel
computation which has been difficult to do for optimization based
methods. Your comment on how use of a CNN to solve the matching
problem appears contrived but we use GNNs - unless there is some
confusion here.
% TODO: We could use sets but that would throw away the structure of our data
% TODO: Most structured 

% TODO: Less apologetic
% TODO: Their comments in quotes
To address your other comments, we did not focus on the mechanics of
GNNs to focus on cycle consistency and the matching graph, however we
can add more to the text to make it clearer. We should reference the
figures more in the text - they are there for illustrative purposes but
we should indeed explicitly cite them. Relatedly, can reference the
figures on the experiments in the text more.
You are right in that we neglected to cite the spectral method
\cite{pachauri2013solving} except briefly in the methods section. This
is simply taking the top $k$ eigenvectors as the embedding. It is the
simplest method to solve mutli-image matching and works perfectly when
there is no noise, but breaks down more quickly when there is more noise.

We admit the lack of larger view-sets for our experiments is an
oversight. However RANSAC based methods start becoming computationally
infeasible at 3 and 4 images (too many points to select) so it appeared
the simplest test to see how our method could function in ways simple
RANSAC based methods wouldn't.

\section{Reviewer \#3}
Thank you for you comments and we agree that this method is interesting
and has many interesting applications.
To address the weaknesses you point out, you are right that an
precision-recall or ROC curve would be a good measure (W1). We have
added in this rebuttal a figure showing the ROC curves of our method
compared to other methods, and we still compare favorably. You are
right in that more outlier experiments could have been done (W2b). 

You say that Rome16K is an inappropriate dataset for testing Cycle
Consistency (W2a). We couldn't find a more appropriate large scale
dataset where we could find ground truth matches. We could collect our
own but we wanted to test against a well-known dataset (even if it is
typically used in other contexts) for a more fair comparison. You are 
right we did not specify how we split training and testing - there was
no overlap of sequences in training and testing sets. It was trained on
one set of buildings and tested on another, so it has not overfit to
particular buildings.

You are right to point out we did not include the implementation
details of our architecture. Our layer splits were: 
32, 64, 128, 256, 512, 512, 512, 512, 512, 512, 1024, 1024.
For your other comments, the first comment was addressed in the last
paragraph of Reviewer \#1, and the others are valid comments that we
will attempt to fix.

\section{Reviewer \#5}
Thank you as well for your comments and feedback.
We agree with you that this topic is an important research area and feel
that our method has much to contribute to that.

You as well as Reviewer \#1 felt the discussion on GNNs was insufficient
and we will add more detail should this paper be accepted. To address
your comments on the scale of the mutil-image experiments, see Reviewer
\#1. We appreciate the comment about the axes of figure and we will
fix that, as well as your other comments on typos and figure fixes.
You mentioned~\cite{hajder2006weak}, which did not come up in our
literature search, but it does address mutli-view robust image matching
and we will add it to our citations. 



\begin{figure}[t]
\begin{center}
\fbox{\rule{0pt}{1.8in} \rule{0.9\linewidth}{0pt}}
   %\includegraphics[width=0.8\linewidth]{egfigure.eps}
\end{center}
   \caption{Example of caption.  It is set in Roman so that mathematics
   (always set in Roman: $B \sin A = A \sin B$) may be included without an
   ugly clash.}
\label{fig:long}
\label{fig:onecol}
\end{figure}

{\small
\bibliographystyle{ieee}
\bibliography{egbib}
}

\end{document}
