
\documentclass{article} % For LaTeX2e
\usepackage{iclr2020_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{subcaption}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{url}

\newcommand{\cross}[1]{[#1]_{\times}}
\setlength{\textfloatsep}{12pt plus 2pt minus 2pt}

\title{Graph Neural Networks For Multi-Image Matching}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Stephen Phillips, Kostas Daniilidis \\
School of Engineering and Applied Sciences \\
University of Pennsylvania \\
Pennsylvania, PA 19104, USA \\
\texttt{\{stephi, kostas\}@seas.upenn.edu} \\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
    Image feature matching is a fundamental part of many geometric computer vision applications, and using multiple images can improve performance.
    In this work, we formulate multi-image matching as a graph embedding problem then use a Graph Neural Network to learn an appropriate embedding function for aligning image features.
    We use cycle consistency to train our network in an unsupervised fashion, since ground truth correspondence is difficult or expensive to acquire.
    In addition, geometric consistency losses can be added at training time, even if the information is not available in the test set, unlike previous approaches which optimize cycle consistency directly.
    To the best of our knowledge, no other works have used graph neural networks for multi-image feature matching.
    Our experiments show that our method is competitive with other optimization based approaches.
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}

Feature matching is an essential part of Structure from Motion and many geometric computer vision applications.
The goal in multi-image feature matching is to take 2D image locations from three or more images and find which ones correspond to the same point in the 3D scene.
Methods such as SIFT feature matching (\cite{lowe2004distinctive}) followed by RANSAC (\cite{fischler1981random}) have been the standard for decades.
However RANSAC-based approaches are limited to matching pairs of images, which can lead to global inconsistencies in the matching.
% TODO: Mention trifocal tensors
Other works, such as Wang et al.~\cite{wang2017multi}, have shown improvement in performance by optimizing cycle consistency, i.e. enforcing the pairwise feature matches to be globally consistent.

However, these multi-view consistency algorithms struggle in distributed and robust settings.
Having image features suited for this task would help improve performance, and deep learning has revolutionized how image features are computed (\cite{yi2016lift}).
In this paper, we want to leverage the power of deep representations in order to compute feature descriptors that are robust across multiple views.
% Typically when applied to image or feature matching, deep neural networks (DNNs) are trained with photometric losses, directly using pixel color values.
% However direct, pixel-based losses have are not robust to the many confounding factors in the environment, such as lighting and view-point changes.
% Using features designed to deal with these confounding factors improves robustness to them, and learning may improve robustness even further.

% \begin{figure}[t]
% \begin{center}
%   % \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
%   \includegraphics[width=0.8\linewidth]{figures-CycleConsistencyBasic-v2.pdf}
% \end{center}
%   \caption{
%     Example of multi-image matching and cycle consistency, with images from Rome16K dataset \cite{li2010location}.
%     Pairwise matches between image can lead to global inconsistencies in the matching.
%     In this example, an error in matching leads to an inconsistent cycle (shown in red), ideally these pairwise matches would be consistent.
%     We improve the quality of pairwise matches in the multi-view matching problem by training a neural net with cycle consistency.
%   }
% \label{fig:cycconsistex}
% \end{figure}

%------------------------------------------------------------------------
\begin{figure*}[t]
\begin{center}
  % \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
  \includegraphics[width=0.8\linewidth]{figures-CycleConsistencyMainFigure-v4.pdf}
\end{center}
  \caption{
    An illustration of the approach of this work.
    The Graph Neural Neural Network (GNN) (\cite{battaglia2018relational}) takes as input the graph of matches and then outputs a low rank embedding of the adjacency matrix of the graph.
    The GNN operates on an embedding over the vertices of the graph.
    In the figure, the GNN vertex embeddings are represented by different colors.
    The final embedding is used to construct a pairwise similarity matrix, which we train to be a low dimensional cycle-consistent representation of the graph adjacency matrix, thus pruning the erroneous matches.
    We train the network using a reconstruction loss on the similarity matrix with the noisy adjacency matrix, and thus do not need ground truth matches.
    In addition, we can use geometric consistency information, such as epipolar constraints, to assist training the network.
  }
\label{fig:pipeline}
\end{figure*}

Unfortunately, there are obstacles to applying multi-view constraints directly to deep learning. 
To train networks, we need a large amount of labeled data.
In the case of multi-image feature matching, one would need hand labeled point correspondences between images, which are difficult and expensive to obtain.
Multi-view constraints are formulated in terms of sparse features, which traditional convolutional neural nets are not designed to handle.
Additionally, in the case of multi-image feature matching, geometric constraints would be helpful in rejecting outlier matches.
Geometric constraints, such as the epipolar constraint, can help disambiguate between visually similar features during training, and help the network learn better features robust to this kind of noise.

In this work, we propose to solve these problems using Graph Neural Networks (GNNs) to operate on the correspondence graph.
The proposed method works directly on the correspondence graph, which is agnostic to how the correspondences were computed, thus allowing the algorithm to work in a broad class of environments.
To the best of our knowledge this work is the first to apply deep learning to the multi-view feature matching problem.
We use an unsupervised loss - the cycle consistency loss - to train the network, and thus avoiding the difficulty of expensive hand labeling.
Geometric consistency losses can aid training, even if such information is not available at test time.
Although our network is simple, it shows promising results compared to baselines which optimize for cycle-consistency without learned embeddings, using a matrix factorization loss (\cite{zhou2015multi, leonardos2016distributed}).
Furthermore, since inference requires only a single forward pass over the neural network, our approach is faster (to achieve comparable accuracy) than methods which must solve an optimization problem every time.
We perform experiments on the Rome16K dataset (\cite{li2010location}) to test the effectiveness of our method compared to optimization based methods.
Our contributions in this work are:
\begin{itemize}
\item We use a novel architecture to address the multi-image feature matching problem using GNNs with graph embeddings.
\item We introduce an unsupervised cycle consistency loss that does not require labeled correspondences to train.
\item We demonstrate the effectiveness of geometric consistency losses in improving training.
\end{itemize}


%-------------------------------------------------------------------------
\section{Related Work}

\subsection{Feature Matching}
Matching has a rich history of research in computer vision.
Much work has be done using hand-crafted feature descriptors such as SIFT (\cite{lowe2004distinctive}), SURF (\cite{bay2006surf}), BRIEF (\cite{calonder2012brief}), or ORB (\cite{mur2015orb}).
RANSAC \cite{fischler1981random} is the most widely used robust estimation technique to filter out outliers from the matches.
The combination of RANSAC and hand-crafted feature descriptors has constituted the bulk of the matching literature for the last 40 years.
Finally, graph matching (\cite{suh2015subgraph, hu2016distributable}) can be used as a final step for more robust matches.

\subsection{Multi-image Matching}
Multi-image matching has traditionally been done using optimization based methods minimizing a cycle consistency based loss (see Section 3.3).
\cite{pachauri2013solving} and \cite{arrigoni2017synchronization} use the eigenvectors of the matching matrix to obtain a low dimensional embedding. 
However, the low Gaussian noise assumption is not realistic.
\cite{zhou2015multi} and \cite{wang2017multi} use most sophisticated optimization techniques on the matching matrix and thus produce more robust solutions.
\cite{leonardos2016distributed} implement a distributed optimization scheme to solve for cycle consistency.
As an alternative to optimization based techniques, \cite{tron2017fast} used density based clustering techniques to compute multi-image correspondence.
Moving away from feature matching, \cite{zach2010disambiguating} uses cycle consistency like constraints on pose graphs quite effectively.
To the best of our knowledge, we are the first to use graph neural networks for multi-image matching.

\subsection{Deep Learning for Matching}
Previous attempts to improve image matching techniques using machine learning have focused on learning the descriptors given ground truth correspondence from curated datasets such as \cite{zagoruyko2015learning}; \cite{yi2016lift}; and \cite{brachmann2017dsac}.
This approach is limited if one do not have the ability to get the ground truth correspondences.
There are other methods to build correspondences such as \cite{choy2016universal}, but they only handle two-view constraints and require dense correspondences.
Most similar to our work, \cite{yi2018learning} attempts to improve correspondences by learning match probabilities for RANSAC for greater robustness and speed.
However, they only focus on two view matching and do not exploit the advantages of the correspondence structure.
Note that while \cite{zhu2017unpaired} use cycle consistency in their loss, their method is restricted to pairwise cycle consistency (i.e. enforcing consistency by going back and forth between two images).
We use multi-image cycle consistency, which require consistency between 3 or more images.

\subsection{Graph Neural Networks}
Graph neural networks have received more attention recently e.g. \cite{bronstein2017geometric, bruna2013spectral, defferrard2016convolutional, kipf2017semi, scarselli2009graph, gama2018mimo, gama2018convolutional}; and \cite{battaglia2018relational}.
Classical so-called Spectral methods used the eigenvectors of the graph Laplacian to compute convolutions as in \cite{bruna2013spectral}, but requires an a-priori known graph structure. 
Non-spectral methods do not require a-priori knowledge, as seen in \cite{bronstein2017geometric, kipf2017semi, scarselli2009graph}; and \cite{gama2018convolutional}.
Most of these methods use polynomials of the graph Laplacian to compute neighborhood averages.
\cite{gama2018mimo, gama2018convolutional} formalize this notion and generalize it beyond the use of the graph Laplacian.
To improve performance, more sophisticated aggregation techniques and global information passing can be used as discussed in \cite{battaglia2018relational}.

\section{Method}

\begin{figure}[t]
\begin{subfigure}[b]{.55\linewidth}
  \centering
  \includegraphics[width=0.95\linewidth]{figures-GeometricConsistency-v2.pdf}
  \caption{Illustrated here is an example of how the geometric loss is computed for one feature.}
  \label{fig:1b}
  \label{fig:geoconsist}
\end{subfigure}
\begin{subfigure}[b]{.30\textwidth}
  \centering
  % \includegraphics[width=0.65\linewidth]{figures-GeometricLoss.pdf}
  \includegraphics[height=180px]{figures-GeometricLoss.pdf}
  \caption{}
  \label{fig:3b}
  \label{fig:geomloss}
\end{subfigure}
\caption{
  \textbf{(a)} Errors are computed via absolute distance from the epipolar line, as expressed by (\ref{eq:essential_constraint}) via the epipolar constraint.
  The epipolar line is the line of projection of the feature on the first image, projected onto to the second.
  The distance to this line on the second image indicates how likely that point is to correspond geometrically to the original feature.
  There can be false positives along the projected line, as shown by the square feature in the figure, but other points will be eliminated, such as the hexagonal feature.
  \textbf{(b)} Training curves with and without Geometric Training loss, described in \ref{eq:geom_cost2}.
  The geometric training loss improves testing performance.
  While the training is higher due to the additional loss terms, the ground truth L1 error is substantially better with the Geometric Loss.
  Best viewed in color.
}
\label{fig:1}
\end{figure}


\begin{figure}[t]
\begin{subfigure}[b]{.45\linewidth}
  \centering
  \includegraphics[width=0.95\linewidth]{figures-UniverseOfFeatures-v2.pdf}
  \caption{Illustration of the universe of features.}
  \label{fig:1a}
  \label{fig:universefeatures}
\end{subfigure}%
\begin{subfigure}[b]{.55\linewidth}
  \centering
  \includegraphics[width=0.75\linewidth]{figures-EmbeddingsViz.png}
  % \includegraphics[height=150px]{figures-EmbeddingsViz.png}
  \caption{}
  \label{fig:1b}
  \label{fig:embeddingsviz}
\end{subfigure}
\caption{
  \textbf{(a)} Each feature in each image corresponds to a 3D point in the scene.
  We can construct cycle consistent embeddings of the features by mapping each one to the one-hot vector of its corresponding 3D point.
  While there can be many features, there are fewer 3D points and thus this corresponds to a low rank factorization of the correspondence matrix.
  \textbf{(b)} Visualization of the learned embeddings.
  On the left we have the raw outputs, which are difficult to interpret.
  In the center, we rotated the features to best match the ground truth for a more interpretable visualization.
  On the right, we have the ground truth embeddings, given as indicator vectors for which feature in the world the points correspond to.
  Best viewed in color.
}
\label{fig:1}
\end{figure}

Our goal is to learn optimal features that capture multiple image views by filtering out noisy feature matches.
An outline of our approach can be seen in figure \ref{fig:pipeline}.
The input to our algorithm is a set of features and noisy correspondences, and the output is a new set of features where the pairwise similarities of these features correspond to the true matches.
We do this by training the new set of feature embeddings to be cycle consistent.
We formulate this problem in terms of the correspondence graph of the features.
Graphs $\mathcal{G} = (\mathcal{V}, \mathcal{E})$ have a set of vertices $\mathcal{V}$ and of edges $\mathcal{E} \subseteq \mathcal{V} \times \mathcal{V}$ and implicitly in the subsequent discussions we assume the edges are directed.
For a vertex $v \in \mathcal{V}$ we use $\mathcal{N}^{h}(v)$ to denote the $h$-hop neighbors of $v$, with the superscript left out for 1-hop neighbors.
Similarly $\mathcal{E}(v)$ is used to denote the edges associated with $v$.
To denote the vertices connected to an edge $e \in \mathcal{E}$ we write $e(v_1, v_2)$.
\subsection{Correspondence Graph}
We assume there is an initial set of feature matches represented as a graph $\mathcal{G} = (\mathcal{V}, \mathcal{E})$, with an associated adjacency matrix $\mA$.
The graph is constructed from putative correspondences of image features across images, typically constructed using feature descriptor distance (e.g. SIFT feature distance).
While there are many interesting methods for computing these putative correspondences (\cite{suh2015subgraph, yi2018learning}), we do not explore them in this work.
Typically putative correspondences are matched probabilistically, meaning a feature in one image matches to many features in another.
The ambiguity in the matches could come from repeated structures in the scene, insufficiently informative low-level feature descriptors, or just an error in the matching algorithm.
Filtering out these noisy matches is our primary learning goal.

Each vertex of the graph $v \in \mathcal{V}$ is an image feature, corresponding to some ground truth 3D point $\vp(v)$.
Each edge $e = (v_1, v_2) \in \mathcal{E}$ is a potential correspondence.
Associated with each vertex $v$ is an embedding $\vf_v \in \sR^m$, which can include the visual feature descriptor, position, scale, orientation, etc.
Similarly, each edge $e$ has an associated feature $\vf_e \in \sR^p$ (in this work, initially just the weight of the feature association).
We use these features as the initialization for our learning algorithm.

In the absence of noise or outliers, this graph would have a connected component for each visible point in the world, all mutually disjoint.
Without noise, vertices $v$ would only match with other vertices $v'$ that correspond to the same 3D point in the scene.
Since features in this case represent unique locations in the scene, no points in the same image would have edges $e$ between them.
Mathematically, this can be expressed as $e = (v_1, v_2) \in \mathcal{E} \implies \mP(v_1) = \mP(v_2)$.
In the noisy case we expect this structure to be corrupted, i.e. there are some edges $e = (v_1, v_2) \in \mathcal{E}$ such that $\mP(v_1) \neq \mP(v_2)$.
Thus we need to prune the erroneous edges.

However, standard CNNs cannot operate on this general graph structure.
Thus we cannot use standard convolutional nets to learn features for this task.
Instead we use graph networks to learn feature representations on this space, which we describe in the next section.

\subsection{Graph Neural Networks}
As input to our method we are given a graph $\mathcal{G} = (\mathcal{V}, \mathcal{E})$ with the features described last section $\vf_v \; \forall v \in \mathcal{V}$ and $\vf_e \; \forall e \in \mathcal{E}$.
As with any neural network, GNNs have layered outputs.
We describe the output of layer $k$ as $\vf_v^{(k)} \in \sR^{m_k} \, \forall v \in \mathcal{V}$ and $\vf_e^{(k)} \in \sR^{p_k}\, \forall e \in \mathcal{E}$, with the initial embeddings denoted $\vf_v^{(0)} = \vf_v$ and $\vf_e^{(0)} = \vf_e$.
It will be useful to write these features as matrices in later sections, so we denote the vertex embedding matrix as $\mF_V^{(k)}$ and the edge embedding matrix as $\mF_E^{(k)}$.
If a superscript is not specified then it refers to the final output of the network.

First we describe older methods of GNNs to give context, then we describe the method we use in this work.
Many older methods assume we have the adjacency matrix $\mA$ of the graph known a-priori \cite{bruna2013spectral}, and can encode graph convolutions using the eigenvectors of $\mA$.
However, we do not have this luxury, as the correspondence structure changes from image set to image set, and thus we use non-spectral Graph Neural Networks.
There are many variations on non-spectral methods, often which ultimately amount to message passing between vertices of the graph with learned non-linear transformations between various steps (\cite{kipf2017semi, defferrard2016convolutional, gama2018mimo, gama2018convolutional}).
Some works such as \cite{gama2019convolutional} use pooling operations on the vertices to make the graph smaller and thus aid computation, but as we need labels on every vertex of the original graph, so we cannot use this.
Most of GNNs used in these works can be expressed mathematically as:
\begin{align*}
\tilde{\vf}_v^{(k+1)} =&\; \sigma\left( b^{(k)} + \mW_0^{k} \vf_{v}^{(k)} + \sum_{h=0}^H \sum_{v' \in \mathcal{N}^{h}(v)} f_{e(v,v')} \mW_{h}^{k} \vf_{v'}^{(k)} \right) \\
\end{align*}
The weights/biases $\mW_{h}^{k}$, $b^{(k)}$ are all learned, with no learning done on the edge weights $f_{e(v,v')}$. 
Note that this is just sums or averages over $h$-hop neighborhoods, where the weights on the edges remain static through the computation.
Given that we are trying to prune edges, it would be sensible to add features over edges to learn which ones to prune and which to keep such as in \cite{scarselli2009graph}.

Therefore, in this work we use the method and implementation described in \cite{battaglia2018relational} (more specifically repetitions of the architecture described in \cite{battaglia2016interaction}).
This work is similar to previous works with one key difference: edges also have weights.
Thus therefore there is intermediate processing on the edges before information is passed to the vertices. 

Mathematically, this is expressed as: % TODO: Maybe make this MLP oriented?
\begin{align}
\tilde{\vf}_{e(v_1,v_2)}^{(k+1)} =&\; \sigma \left(a^{(k)} + \mU_1^{(k)} \vf_{e}^{(k)} + \mU_1^{(k)} \vf_{v_1}^{(k)} + \mU_2^{(k)} \vf_{v_2}^{(k)} \right) \\
\tilde{\vf}_{v}^{(k+1)} =&\; \sigma\left(b^{(k)} + \mW_0^{(k)} \vf_{v}^{(k)} + \sum_{e \in \mathcal{E}(v)} \mW_1^{(k)} \vf_{e}^{(k+1)} \right) 
\end{align}
Here the learned weights are denoted $\mW$ and $\mU$, and the biases $a^{(k)}$ and  $b^{(k)}$.
In \cite{battaglia2018relational}, they allow for more sophisticated aggregation functions, but in this work we simply use the mean function.
In practice, we use MLPs in message passes between vertices and edges for better expressiveness


\subsection{Cycle Consistency}

Let $\mM$ be the noiseless set of matches between our features, with $\mM_{ij}$ being the partial permutation representing the matches between image $i$ and image $j$.
If the pairwise matches are globally consistent, then for all $i, j, k$:
\begin{equation}
\mM_{ij} = \mM_{ik} \mM_{kj}
\label{eq:cycconsist1}
\end{equation}
In other words, the matches between two images stay the same no matter what path is taken to get there. 
This constraint is known as \textit{cycle consistency}, and has been used in a number of works to optimize for global consistency (\cite{zhou2015multi, wang2017multi, leonardos2016distributed}).
Stated in this form, there are $O(n^3)$ cycle consistency constraints to check.
A more elegant way to represent cycle consistency is to first create a `universe' of features that all images match to (see figure \ref{fig:universefeatures}).
Then, one can match the $i^{th}$ set of features to the universe using a ground-truth matching matrix $\mX_i$.
Then the cycle consistency constraint becomes:
\begin{equation}
\mM_{ij} = \mX_{i}\mX_{j}^\top
\label{eq:cycconsist2}
\end{equation}


\begin{table*}[t]
\begin{center}
\begin{tabular}{|l|c|c|}
\hline
Method & Same Point Similarities & Different Point Similarities  \\
\hline\hline\hline
Ideal                              & 1.00e+0 $\pm$ 0.00e+0 & 0.00e+0 $\pm$ 0.00e+0 \\ \hline
Initialization Baseline            & 5.11e-1 $\pm$ 1.68e-2 & 2.56e-1 $\pm$ 2.06e-1 \\ \hline
3 Views, Noiseless                 & 9.96e-1 $\pm$ 7.70e-3 & 1.16e-1 $\pm$ 1.32e-1 \\ 
5 Views, Noiseless                 & 1.00e+0 $\pm$ 4.15e-4 & 1.22e-1 $\pm$ 1.67e-1 \\ \hline
3 Views, Added Noise               & 9.96e-1 $\pm$ 7.70e-3 & 1.16e-1 $\pm$ 1.32e-1 \\ 
5 Views, Added Noise               & 9.89e-1 $\pm$ 2.47e-2 & 7.67e-2 $\pm$ 1.56e-1 \\ 
6 Views, Added Noise               & 9.84e-1 $\pm$ 3.16e-2 & 7.46e-2 $\pm$ 1.57e-1 \\ \hline
3 Views, 5\% Outliers              & 9.29e-1 $\pm$ 1.79e-1 & 1.41e-1 $\pm$ 1.48e-1 \\ 
3 Views, 10\% Outliers             & 9.27e-1 $\pm$ 1.79e-1 & 1.40e-1 $\pm$ 1.51e-1 \\ \hline

\hline
\end{tabular}
\end{center}
\caption{
Results on Synthetic correspondence graphs.
The `Same Point Similarities' column is the mean and standard deviation of similarities for true corresponding points, while the `Different Point Similarities' is the same for points that do not correspond.
For the `Same Point Similarities' column higher is better, and for `Different Point Similarities' lower is better.
Losses tested against ground truth correspondence graph adjacency matrices.
Our method was not trained on ground truth correspondences but using unsupervised methods.
}
\label{fig:synthtable}
\end{table*}

This reduces the number of our constraints from $O(n^3)$ to $O(n^2)$.
We try to learn vertex embeddings $\mF_V$ to approximate $\mX$ - in other words the final embedding should be an encoding of the universe of features.
As we do not have the ground truth matches $\mM$, we approximate it using the noisy adjacency matrix $\mA$ of our correspondence graph. Thus our loss would be 
\begin{equation}
\mathcal{L}(\mA, \mF_V) = \mathcal{D}(\mA, \mF_V \mF_V^\top)
\end{equation}
Here $\mathcal{D}$ could be an $L_2$ loss, $L_1$ loss, or many others. In this work, we use the $L_1$ loss. 
Note that because of this formulation, we can determine our embeddings only up to a rotation, as
$ \mF_V R(\mF_V R)^\top
= \mF_V RR^\top \mF_V^\top
= \mF_V\mF_V^\top
\label{eq:rotinvar} $
Thus when visualizing embeddings, we rotate them to make them more interpretable (see figure \ref{fig:embeddingsviz}).

\subsection{Geometric Consistency Loss}

One of the main advantages of this approach over more traditional optimization based approaches is the ability to add geometric consistency information into the loss at training time, even if it is not available at test time.
The simplest way to add geometric consistency losses, and the approach we use here, is to use the epipolar constraint.
The epipolar constraint describes how the positions of features in different images corresponding to the same point should be related.
An illustration of this is provided in figure \ref{fig:geoconsist}, showing how this loss can help reject erroneous points.
Given a relative pose $(R_{ij}, T_{ij})$ between two cameras $i$ and $j$  (transforms $j$ to $i$) the epipolar on corresponding feature locations $X_i$ and $X_j$: $X_{i}^\top \cross{T_{ij}}R_{ij} X_{j} = 0$.
In this work we use the two pose epipolar constraint (\cite{tron2014quotient}):
\begin{equation}
X_{i}^\top R_{i}^\top \cross{T_{j} - T_{i}}R_{j} X_{j} = 0
\label{eq:essential_constraint}
\end{equation}
% Where the $(R_k, T_k)$ are the poses of cameras $i$ and $j$ respectively.
The constraint assumes that the $X_k$ are calibrated (i.e. the camera intrinsics are known). 
Given our vertex embeddings matrix $\mF_V$, we can formulate a loss between all cameras $i$ and $j$:
\begin{align}
\mathcal{L}_{ij,geom}(\mF_V) = 
&\sum_{v \in \mV(i),u \in \mathcal{V}(j)} (\vf_v \cdot \vf_u) \left|X_{v}^\top R_{i}^\top \cross{T_{j} - T_{i}}R_{c_j} X_{u}\right|
\label{eq:geom_cost}
\end{align}
With $\mathcal{V}(i)$ being the vertices associated with camera $i$.
For our purposes, since we use low rank embeddings $\mF_{V, i}$, $\mF_{V, j}$, the loss would read (where $c(k)$ is the appropriate camera for point index $k$):
\begin{align} 
\mathcal{L}_{geom}(\mF_V)
=&\; \mathrm{tr}(\mG^\top \mF_V\mF_V^\top) = \sum_{k,l} (\mF_V)_{k} \cdot (\mF_V)_{l} (\mG)_{kl} \label{eq:geom_cost2} \\
(\mG)_{kl} =&\; \left|X_{k}^\top R_{c(k)}^\top \cross{T_{c(l)} - T_{c(k)}}R_{c(l)} X_{l}\right| \nonumber
\end{align}



\begin{table*}[t]
\begin{center}
\begin{tabular}{|l|c|c|c|c|}
\hline
Method (6 Views)                                     & $L_1$             & $L_2$             & Area under ROC    & Time (sec)        \\
\hline\hline
MatchALS 15 Iterations & 0.101 $\pm$ 0.008 & 0.022 $\pm$ 0.004 & 0.918 $\pm$ 0.073 & 0.074 $\pm$ 0.008 \\ \hline
MatchALS 35 Iterations & 0.046 $\pm$ 0.016 & 0.010 $\pm$ 0.005 & 0.910 $\pm$ 0.072 & 0.139 $\pm$ 0.041 \\ \hline
MatchALS 50 Iterations & 0.029 $\pm$ 0.017 & 0.008 $\pm$ 0.005 & 0.905 $\pm$ 0.068 & 0.260 $\pm$ 0.048 \\ \hline
PGDDS0 15 Iterations   & 0.017 $\pm$ 0.002 & 0.007 $\pm$ 0.001 & 0.918 $\pm$ 0.087 & 0.796 $\pm$ 0.147 \\ \hline
PGDDS0 25 Iterations   & 0.016 $\pm$ 0.002 & 0.007 $\pm$ 0.002 & 0.919 $\pm$ 0.087 & 1.670 $\pm$ 0.328 \\ \hline
PGDDS0 50 Iterations   & 0.015 $\pm$ 0.002 & 0.006 $\pm$ 0.002 & 0.920 $\pm$ 0.087 & 3.363 $\pm$ 0.528 \\ \hline
Spectral               & 0.073 $\pm$ 0.006 & 0.027 $\pm$ 0.003 & 0.921 $\pm$ 0.083 & 0.036 $\pm$ 0.005 \\ \hline
\textbf{GNN (ours)}    & 0.044 $\pm$ 0.005 & 0.031 $\pm$ 0.005 & 0.872 $\pm$ 0.081 & 0.765 $\pm$ 0.046 \\ \hline

\hline\hline
Method (10 Views)                                   & $L_1$             & $L_2$             & Area under ROC    & Time (sec)        \\
\hline\hline
MatchALS 25 Iterations & 0.092 $\pm$ 0.008 & 0.019 $\pm$ 0.003 & 0.912 $\pm$ 0.055 & 0.228 $\pm$ 0.014 \\ \hline
MatchALS 35 Iterations & 0.065 $\pm$ 0.009 & 0.013 $\pm$ 0.003 & 0.907 $\pm$ 0.053 & 0.355 $\pm$ 0.073 \\ \hline
MatchALS 50 Iterations & 0.045 $\pm$ 0.012 & 0.011 $\pm$ 0.004 & 0.914 $\pm$ 0.051 & 0.455 $\pm$ 0.022 \\ \hline
PGDDS 15 Iterations    & 0.017 $\pm$ 0.001 & 0.008 $\pm$ 0.001 & 0.903 $\pm$ 0.061 & 1.225 $\pm$ 0.159 \\ \hline
PGDDS 25 Iterations    & 0.016 $\pm$ 0.001 & 0.007 $\pm$ 0.001 & 0.904 $\pm$ 0.061 & 2.637 $\pm$ 0.357 \\ \hline
PGDDS 50 Iterations    & 0.016 $\pm$ 0.001 & 0.007 $\pm$ 0.001 & 0.905 $\pm$ 0.061 & 6.116 $\pm$ 1.009 \\ \hline
Spectral               & 0.073 $\pm$ 0.005 & 0.029 $\pm$ 0.002 & 0.912 $\pm$ 0.057 & 0.081 $\pm$ 0.021 \\ \hline
\textbf{GNN (ours)}    & 0.053 $\pm$ 0.006 & 0.035 $\pm$ 0.005 & 0.872 $\pm$ 0.061 & 2.438 $\pm$ 0.070 \\ \hline

\end{tabular}
\end{center}

\caption{
Results on Rome16K Correspondence graphs, showing the mean and standard deviation of the $L_1$ and $L_2$.
Our method was not trained on ground truth correspondences but using unsupervised methods and geometric side losses.
Thus we test against ground truth correspondence graph adjacency matrices computed from the bundle adjustment output.
Our method performs better than 25 iteration of the MatchALS (\cite{zhou2015multi}) method, but does not perform as well as 50 iterations.
% Our network has only 12 layers, thus we have much greater efficiency per iteration.
We do not perform as well as the Projected Gradient Descent Doubly Stochastic (PGDDS) (\cite{leonardos2016distributed}) but we perform significantly faster than them.
We perform better than a simple eigenvalue based method (\cite{pachauri2013solving}).
Note that we perform much better in $L_1$ performance rather than $L_2$, as we optimized the network weights using an $L_1$ loss.
}
\label{tab:rome16ktab}
\end{table*}



%------------------------------------------------------------------------
\section{Experiments}

% \begin{figure*}
% \begin{center}
%   % \fbox{\rule{0pt}{2in} \rule{.9\linewidth}{0pt}}
%   \includegraphics[width=0.8\linewidth]{figures-ExampleOutput.pdf}
%   \end{center}
%      \caption{Example output of our network. (a) Similarity Matrix of the of the embeddings (b) Histogram of feature similarities for pairs which correspond to the same 3D point, comparing our output features with the original SIFT features (c) Histogram of feature similarities for pairs which correspond to different 3D points, comparing our output features with the original SIFT features}
%   \label{fig:short}
% \end{figure*}

\subsection{Synthetic Graph Dataset}
We first test our method on synthetically generated data as a simple proof of concept.
As these were simpler datasets, we the simpler edge-feature free model of (\cite{kipf2017semi}).
To generate the data, we generate $p$ points, each with its own randomly generated descriptor.
To create the graph, we generate random permutation matrices, with a noise applied to it after it is generated.
% The initial descriptors we use as input we created using the true descriptor plus some added Gaussian noise.
We initialize the input descriptors using the true descriptor, plus some added Gaussian noise.
No geometric losses were added during training for these experiments.
However, the method was robust in testing with different noise functions and parameters.
The normalized noisy input descriptors are our baseline - they correlate with the true values but do not preserve the structure well.
However, the GNN recovered the true structure very well, as shown in Table \ref{fig:synthtable}.
All experiments were run with a 12 layer GNN with the ReLU nonlinearity and skip connections.
The feature vector lengths were 32, 64, 128, 256, 512, 512, 512, 512, 512, 512, 1024, 1024, with skip connections between layers 1 and 6, 6 and 12, and 1 and 12.
All were trained with the Adam optimizer (\cite{kingma2014adam}) and a learning rate $10^{-4}$
The network was implemented in Tensorflow (\cite{tensorflow2015}), version 1.11.
With this simple test on synthetic data passed, we now move to more challenging datasets.

\begin{figure}[t]
\centering
\includegraphics[width=0.95\linewidth]{figures-error_lines.pdf}
% \includegraphics[height=200px]{figures-error_lines.pdf}
\caption{
  Plot of the losses of the baselines at different iteration numbers.
  The line shows the mean of the graph while the translucent coloring shows the $25^{th}$ to $75^{th}$ percentiles.
  The ROC AUC curves remain fairly consistent while the L1 loss goes noticibly down after more iterations.
  Our method compares to 35-45 iterations of MatchALS, while only having 16 layers and 8 message passes.
  PGDDS performs better than us in $L_1$ but we perform similarly in the ROC AUC metric.
  These results still hold even when we change domains to the Graffiti dataset (see \ref{sec:graffiti}).
}
\label{fig:3}
\label{fig:errorlines}
\end{figure}

% 
% \begin{figure}[t]
% \centering
% \begin{subfigure}[b]{.70\textwidth}
%   \centering
%   % \includegraphics[width=0.95\linewidth]{figures-error_lines.pdf}
%   \includegraphics[height=200px]{figures-error_lines.pdf}
%   \caption{}
%   \label{fig:3a}
%   \label{fig:errorlines}
% \end{subfigure}
% \begin{subfigure}[b]{.25\textwidth}
%   \centering
%   % \includegraphics[width=0.65\linewidth]{figures-GeometricLoss.pdf}
%   \includegraphics[height=200px]{figures-GeometricLoss.pdf}
%   \caption{}
%   \label{fig:3b}
%   \label{fig:geomloss}
% \end{subfigure}
% \caption{
%   \textbf{(a)} Plot of the losses of the baselines at different iteration numbers.
%   The line shows the mean of the graph while the translucent coloring shows the $25^{th}$ to $75^{th}$ percentiles.
%   The ROC AUC curves remain fairly consistent while the L1 loss goes noticibly down after more iterations.
%   Our method compares to 35-45 iterations of MatchALS, while only having 16 layers and 8 message passes.
%   PGDDS performs better than us in $L_1$ but we perform similarly in the ROC AUC metric.
%   \textbf{(b)} Training curves with and without Geometric Training loss, described in \ref{eq:geom_cost2}.
%   The geometric training loss improves testing performance.
%   While the training is higher due to the additional loss terms, the ground truth L1 error is substantially better with the Geometric Loss.
%   Best viewed in color.
% }
% \label{fig:3}
% \end{figure}

\subsection{Rome 16K Graph Dataset}
We  use the Rome16K dataset (\cite{li2010location}) to test our algorithm in real world settings.
Rome16K consists of 16 thousand images of various historical sites in Rome extracted from Flickr, along with the 3D structure of the sites provided by bundle adjustment.
While not a standard dataset to test cycle consistency, other datasets had insufficient data to train a network on.
Rome16K is typically used to test bundle adjustment methods.
Therefore, to use our method, we extract 6-tuples and 10-tuples of images with overlap of 80 points or more to test our algorithm, with the points established as corresponding in the given bundle adjustment output.
For the initial embedding we use the original 128 dimensional SIFT descriptors, normalized to have unit $L_2$ norm, the calibrated x-y position, the orientation, and log scale of the SIFT feature.
To construct the graph, we take each feature as a vertex and create edges to the 5 nearest SIFT descriptors for the other images.

For these experiments we train with the $L_1$ norm and geometric consistency losses.
We evaluate on a test set using the ground truth adjacency matrix, which we compute from the bundle adjustment given by the Rome16K dataset.
However, we do not train with the ground truth adjacency matrix, only with the noisy adjacency matrix from the graph.
We also add the geometric loss (\ref{eq:geom_cost2}) which helps improve testing performance (see figure \ref{fig:geomloss})
We use the $L_1$ and ROC AUC metrics to measure performance.
For this method to work, we need the dimension of the embedding to be at least the number of unique points in the scene.
Picking the correct number is difficult a-priori, and is a problem with all cycle consistency based methods.
Here we use the ground truth dimension of the embedding to test both our method and the baselines.

The network was implemented using the code provided by \cite{battaglia2018relational} using Tensorflow 1.11 (\cite{tensorflow2015}).
Our network has 16 layers, with 8 message passing operations placed every other layer.
All layers were simple Multi-layer Perceptrons, with no batch norm.
The network was trained with the Adam optimizer (\cite{kingma2014adam}) with a learning rate of $10^{-4}$, with an exponentially decaying learning rate.
We incorporate skip connections between the input, $6^{th}$, and $12^{th}$ layers (all possible pairs). 

% Our method performs better than nearly 
We compare our method to spectral and optimization based baselines with different maximum iteration cutoffs.
Figure \ref{fig:errorlines} illustrates this by plotting the means of various metrics and their $25^{th}$ and $75^{th}$ percentiles, with table \ref{tab:rome16ktab} giving the exact numbers.
Our network, though only using 8 message passes, has comparable accuracy to MatchALS (\cite{zhou2015multi}) run 35 to 45 iterations, with an equivalent message passing step at each phase.
Although our method does not outperform the Projected Gradient Descent - Doubly Stochastic (PGDDS) (\cite{leonardos2016distributed}) method, we perform comparably to them in the ROC AUC metric.
% Our network, though only 12 layers, has comparable accuracy to MatchALS \cite{zhou2015multi} run between 25 and 50 iterations.
% Additional comparisons to different iterations numbers of iterations can be found in the supplementary material.

\subsubsection{Graffiti Dataset} \label{sec:graffiti}
We also run our trained model on the Graffiti Dataset (formatted to be able to be input to our model properly). The results are shown in \ref{fig:errorlines} in the rightmost figure. As the graffiti dataset is very small (only 6 views total), we were not able to train on it. We randomly permute the intra-image order of the features to add some variance - by design the GNN outputs the same result each time, while the optimization methods have a very small amount of variance. The transferred results of Graffiti are similar to the test error of Rome16K - smaller $L_1$ error and comparable ROC error. This shows that the GNNs trained in Rome16K generalize similarly to the optimization based methods.

%------------------------------------------------------------------------
\section{Conclusion}

We have shown a novel method for training feature matching using GNNs, using an unsupervised cycle consistency loss and geometric consistency losses.
We have demonstrated has comparable the traditional optimization based baselines on a simple GNN, while still allowing for end-to-end training integration in deep learning pipelines.
For future work, we will investigate robust losses for better outlier rejection, and using higher order geometric constraints, such as the tri-focal tensor, as additional loss terms.
% A long term goal is to incorporate learning image level features into this pipeline.
% Moreover we hope to adapt this method to a distributed setting, which would be a simple extention given current approach.
With this new architecture, we have the capability of training multi-image matching pipelines end to end, thus allowing us to train for image features explicitly for this task.
We can also extend this to distributed settings where we can train for matching images from multiple distributed agents. 

% \subsubsection*{Acknowledgements}
% Support by ARL DCIST CRA W911NF-17-2-0181, NSF-IIS-1703319, ARL RCTA W911NF-10-2-0016, ONR N00014-17-1-2093, and the Honda Research Institute is gratefully acknowledged.
% 


\bibliographystyle{iclr2020_conference}
\bibliography{egbib}

% \appendix
% \section{Appendix}
% You may include other additional sections here. 

\end{document}


% \begin{table*}[t]
% \begin{center}
% \begin{tabular}{|l|c|c|c|c|}
% \hline
% Method (6 Views)                                     & $L_1$             & $L_2$             & Area under ROC    & Time (sec)        \\
% \hline\hline
% MatchALS 15 Iterations & 0.101 $\pm$ 0.008 & 0.022 $\pm$ 0.004 & 0.918 $\pm$ 0.073 & 0.074 $\pm$ 0.008 \\ \hline
% MatchALS 25 Iterations & 0.059 $\pm$ 0.010 & 0.012 $\pm$ 0.004 & 0.921 $\pm$ 0.077 & 0.143 $\pm$ 0.064 \\ \hline
% MatchALS 35 Iterations & 0.046 $\pm$ 0.016 & 0.010 $\pm$ 0.005 & 0.910 $\pm$ 0.072 & 0.139 $\pm$ 0.041 \\ \hline
% MatchALS 50 Iterations & 0.029 $\pm$ 0.017 & 0.008 $\pm$ 0.005 & 0.905 $\pm$ 0.068 & 0.260 $\pm$ 0.048 \\ \hline
% PGDDS0 15 Iterations   & 0.017 $\pm$ 0.002 & 0.007 $\pm$ 0.001 & 0.918 $\pm$ 0.087 & 0.796 $\pm$ 0.147 \\ \hline
% PGDDS0 25 Iterations   & 0.016 $\pm$ 0.002 & 0.007 $\pm$ 0.002 & 0.919 $\pm$ 0.087 & 1.670 $\pm$ 0.328 \\ \hline
% PGDDS0 50 Iterations   & 0.015 $\pm$ 0.002 & 0.006 $\pm$ 0.002 & 0.920 $\pm$ 0.087 & 3.363 $\pm$ 0.528 \\ \hline
% Spectral               & 0.073 $\pm$ 0.006 & 0.027 $\pm$ 0.003 & 0.921 $\pm$ 0.083 & 0.036 $\pm$ 0.005 \\ \hline
% \textbf{GNN (ours)}    & 0.044 $\pm$ 0.005 & 0.031 $\pm$ 0.005 & 0.872 $\pm$ 0.081 & 0.765 $\pm$ 0.046 \\ \hline
% 
% \hline\hline
% Method (10 Views)                                   & $L_1$             & $L_2$             & Area under ROC    & Time (sec)        \\
% \hline\hline
% MatchALS 15 Iterations & 0.114 $\pm$ 0.008 & 0.028 $\pm$ 0.004 & 0.915 $\pm$ 0.051 & 0.142 $\pm$ 0.009 \\ \hline
% MatchALS 25 Iterations & 0.092 $\pm$ 0.008 & 0.019 $\pm$ 0.003 & 0.912 $\pm$ 0.055 & 0.228 $\pm$ 0.014 \\ \hline
% MatchALS 35 Iterations & 0.065 $\pm$ 0.009 & 0.013 $\pm$ 0.003 & 0.907 $\pm$ 0.053 & 0.355 $\pm$ 0.073 \\ \hline
% MatchALS 50 Iterations & 0.045 $\pm$ 0.012 & 0.011 $\pm$ 0.004 & 0.914 $\pm$ 0.051 & 0.455 $\pm$ 0.022 \\ \hline
% PGDDS 15 Iterations    & 0.017 $\pm$ 0.001 & 0.008 $\pm$ 0.001 & 0.903 $\pm$ 0.061 & 1.225 $\pm$ 0.159 \\ \hline
% PGDDS 25 Iterations    & 0.016 $\pm$ 0.001 & 0.007 $\pm$ 0.001 & 0.904 $\pm$ 0.061 & 2.637 $\pm$ 0.357 \\ \hline
% PGDDS 50 Iterations    & 0.016 $\pm$ 0.001 & 0.007 $\pm$ 0.001 & 0.905 $\pm$ 0.061 & 6.116 $\pm$ 1.009 \\ \hline
% Spectral               & 0.073 $\pm$ 0.005 & 0.029 $\pm$ 0.002 & 0.912 $\pm$ 0.057 & 0.081 $\pm$ 0.021 \\ \hline
% \textbf{GNN (ours)}    & 0.053 $\pm$ 0.006 & 0.035 $\pm$ 0.005 & 0.872 $\pm$ 0.061 & 2.438 $\pm$ 0.070 \\ \hline
% 
% \end{tabular}
% \end{center}
% 
% \caption{
% Results on Rome16K Correspondence graphs, showing the mean and standard deviation of the $L_1$ and $L_2$.
% Our method was not trained on ground truth correspondences but using unsupervised methods and geometric side losses.
% Thus we test against ground truth correspondence graph adjacency matrices computed from the bundle adjustment output.
% Our method performs better than 25 iteration of the MatchALS (\cite{zhou2015multi}) method, but does not perform as well as 50 iterations.
% % Our network has only 12 layers, thus we have much greater efficiency per iteration.
% We do not perform as well as the Projected Gradient Descent Doubly Stochastic (PGDDS) (\cite{leonardos2016distributed}) but we perform significantly faster than them.
% We perform better than a simple eigenvalue based method (\cite{pachauri2013solving}).
% Note that we perform much better in $L_1$ performance rather than $L_2$, as we optimized the network weights using an $L_1$ loss.
% }
% \label{tab:rome16ktab}
% \end{table*}





