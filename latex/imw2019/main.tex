\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{mycvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{float}
% Include other packages here, before hyperref.
% \usepackage[outdir=./]{epstopdf}

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{3510} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

%% Custom control sequences
\let\oldPr\Pr
\newcommand{\bR}{\mathbb{R}}
\renewcommand*{\Pr}[1]{\mathbb{P}\left( #1 \right)}
\newcommand{\mat}[1]{\mathbf{#1}}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\newcommand{\tr}{\mathbf{tr}}
\newcommand{\cross}[1]{[#1]_{\times}}


% Pages are numbered in submission mode, and unnumbered in camera-ready
%\ifcvprfinal\pagestyle{empty}\fi
\setcounter{page}{1}
\begin{document}

%%%%%%%%% TITLE
% \title{All Graphs Lead to Rome: \\
% Cycle Consistent Multi-View Representations Using Graph CNNs}
\title{All Graphs Lead to Rome: Learning Geometric and Cycle-Consistent Representations with Graph Convolutional Networks}

\author{Stephen Phillips, Kostas Daniilidis \\
GRASP Laboratory, University of Pennsylvania\\
{\tt\small \{stephi, kostas\}@seas.upenn.edu}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
% \and
% Second Author\\
% Institution2\\
% First line of institution2 address\\
% {\tt\small secondauthor@i2.org}
}

% When placing figures in \LaTeX, it's almost always best to use
% \verb+\includegraphics+, and to specify the  figure width as a multiple of
% the line width as in the example below
% {\small\begin{verbatim}
%    \usepackage[dvips]{graphicx} ...
%    \includegraphics[width=0.8\linewidth]
%                    {myfile.eps}
% \end{verbatim}
% }


\maketitle
%\thispagestyle{empty}

% Papers, excluding the references section,
% must be no longer than eight pages in length. The references section
%%%%%%%%% ABSTRACT
\begin{abstract}
%     Image feature matching is a fundamental part of many geometric computer vision applications, and using multiple images can improve performance.
    We formulate multi-image matching as a graph embedding problem and then use a Graph Neural Network to learn an appropriate embedding function for aligning image features.
    We use cycle consistency to train our network in an unsupervised fashion since ground truth correspondence is difficult or expensive to acquire.
%     To the best of our knowledge, no other works have used learning for multi-image feature matching.
%     Our experiments show that our method is competitive with other optimization based approaches.
\end{abstract}
% In addition, geometric consistency losses can be added at training time, even if the information is not available in the test set, unlike previous approaches which optimize cycle consistency directly.
\vspace{-1em}

\section{Introduction}
Feature matching is an essential part of Structure from Motion and many geometric computer vision applications.
The goal in multi-image feature matching is to take 2D image locations from three or more images and find which ones correspond to the same point in the 3D scene.
Works such as Wang et al.~\cite{wang2017multi}, have shown improvement in performance by optimizing cycle consistency, i.e. enforcing the pairwise feature matches to be globally consistent.
In this paper, we want to leverage the power of deep representations in order to learn feature descriptors that are robust across multiple views.

% However, these multi-view consistency algorithms struggle in distributed and robust settings.
% Having image features suited for this task would help improve performance, and deep learning has revolutionized how image features are computed \cite{yi2016lift}.

In this work, we propose to training a Graph Neural Networks (GNNs) to operate directly on the sparse correspondence graph, allowing it to work in a broad class of environments.
To the best of our knowledge this work is the first to apply deep learning to the multi-view feature matching problem.
We use an unsupervised loss - the cycle consistency loss - to train the network, and thus avoiding the difficulty of expensive hand labeling.
Geometric consistency losses can aid training, even if such information is not available at test time.
We compare our method with optimization based methods using a matrix factorization loss \cite{zhou2015multi, leonardos2016distributed}.
We perform experiments on the Rome16K \cite{li2010location} dataset to test the effectiveness of our method compared to optimization based methods.
% Our contributions in this work are:
% \begin{itemize}
% \item We use a novel architecture to address the multi-image feature matching problem using GNNs with graph embeddings.
% \item We introduce an unsupervised cycle consistency loss that does not require labeled correspondences to train.
% \item We demonstrate the effectiveness of geometric consistency losses in improving training.
% \end{itemize}

%------------------------------------------------------------------------
\begin{figure}[t]
\begin{center}
  % \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
  \includegraphics[width=0.9\linewidth]{figures-CycleConsistencyMainFigure-v3.pdf}
\end{center}
  \caption{
    The GNN \cite{battaglia2018relational} takes as input the graph of matches with vertex features and outputs an cycle consistent embedding over the vertices of the graph (represented by different colors in the figure).
  }
\label{fig:pipeline}
% \label{fig:onecol}
\vspace{-1em}
\end{figure}

\begin{figure*}[h]
\begin{center}
  % \fbox{\rule{0pt}{2in} \rule{.9\linewidth}{0pt}}
  \includegraphics[width=0.95\linewidth]{figures-error_lines-v2.pdf}
  \end{center}
     \caption{
         Plot of the losses of the baselines at different iteration numbers.
         The line shows the mean of the graph while the translucent coloring shows the $25^{th}$ to $75^{th}$ percentiles.
         The ROC AUC curves remain fairly consistent while the L1 loss goes noticibly down after more iterations.
         Our method compares to 35-45 iterations of MatchALS, while only having 16 layers and 8 message passes.
         PGDDS performs better than us in $L_1$ but we perform similarly in the ROC AUC metric.
     }
  % \label{fig:twocol}
  \label{fig:errorlines}
\end{figure*}


\section{Method}
Our goal is to learn optimal features that capture multiple image views by filtering out noisy feature matches.
An outline of our approach can be seen in figure \ref{fig:pipeline}.
We formulate this problem in terms of the correspondence graph of the features.

We assume there is an initial set of feature matches represented as a graph $\mathcal{G} = (\mathcal{V}, \mathcal{E})$, with an associated adjacency matrix $\mat{A}$.
Each vertex of the graph $v \in \mathcal{V}$ is an image feature, corresponding to some ground truth 3D point $\mat{P}(v)$.
Each edge $e = (v_1, v_2) \in \mathcal{E}$ is a potential correspondence.
The graph is constructed from putative correspondences of image features across images, typically constructed using feature descriptor distance (e.g. SIFT feature distance).
Each vertex $v$ and edge $e$ has an embedding $\mat{f}_v \in \bR^m$ and $\mat{f}_e \in \bR^p$, (e.g. SIFT Descriptor and distances).

In the absence of noise or outliers, this graph would have a connected component for each visible point in the world, all mutually disjoint.
Mathematically, this can be expressed as $e = (v_1, v_2) \in \mathcal{E} \implies \mat{P}(v_1) = \mat{P}(v_2)$.
Typically putative correspondences are matched probabilistically, meaning a feature in one image matches to many features in another.
Thus in the noisy case we expect this structure to be corrupted, i.e. there are some edges $e = (v_1, v_2) \in \mathcal{E}$ such that $\mat{P}(v_1) \neq \mat{P}(v_2)$.

While there are many types of GNNs \cite{bronstein2017geometric, bruna2013spectral, defferrard2016convolutional, kipf2017semi, scarselli2009graph, gama2018mimo, gama2018convolutional} we use the method and code from \cite{battaglia2018relational} (more specifically repetitions of the architecture described in \cite{battaglia2016interaction}). 
The model we use can be expressed mathematically as:
\begin{align}
\mat{\tilde{f}}_{e(v_1,v_2)}^{(k+1)} =&\; \mat{U}_1^{(k)} \mat{f}_{e}^{(k)} + \mat{U}_1^{(k)} \mat{f}_{v_1}^{(k)} + \mat{U}_2^{(k)} \mat{f}_{v_2}^{(k)} \\
\mat{f}_{e(v_1,v_2)}^{(k+1)} =&\; \sigma \left(a^{(k)} + \mat{\tilde{f}}_{e(v_1,v_2)}^{(k+1)}\right) \\
\mat{\tilde{f}}_{v}^{(k+1)} =&\; \mat{W}_0^{(k)} \mat{f}_{v}^{(k)} + \sum_{e \in \mathcal{E}(v)} \mat{W}_1^{(k)} \mat{f}_{e}^{(k+1)} \\
\mat{f}_v^{(k+1)} =&\; \sigma\left(b^{(k)} + \mat{\tilde{f}}_v^{(k+1)}\right)
\end{align}
Here the learned weights are denoted $\mat{W}$ (for the vertices) and $\mat{U}$ (for the edges), and the biases $a^{(k)}$ and  $b^{(k)}$.
The learned weights on the edges allow the network to express which edges are valuable to keep and which are not.

\subsection{Losses}

We earlier described a necessary condition for a graph to be globally consistent, $e = (v_1, v_2) \in \mathcal{E} \implies \mat{P}(v_1) = \mat{P}(v_2)$.
Another way to describe this is that matching a feature to another image must be path independent in the graph of matches.
This constraint is known as \textit{cycle consistency}, and has been used in a number of works to optimize for global consistency \cite{zhou2015multi, wang2017multi, leonardos2016distributed}.
Naively, there are $O(n^3)$ cycle consistency constraints to check, however a more elegant way to represent cycle consistency is to first create a `universe' of features that all images match to i.e. the 3D points, reducing the number of constraints to $O(n^2)$.
Then, one can match the $i^{th}$ set of features to the universe using a ground-truth matching matrix $\mat{X}_i$.
As we do not have the ground truth matches $\mat{M}$ or embeddings $\mat{X}$, we approximate the matches using the noisy adjacency matrix $\mat{A}$ of our correspondence graph and the matches with the learned representation $\mat{F}_V$.
Then the cycle consistency loss becomes:
\begin{equation}
\mathcal{L}(\mat{A}, \mat{F}_V) = \mathcal{D}(\mat{A}, \mat{F}_V \mat{F}_V^\top)
\end{equation}
Here $\mathcal{D}$ could be an $L_2$ loss, $L_1$ loss, or many others. In this work, we use the $L_1$ loss. 


% The epipolar constraint describes how the positions of features in different images corresponding to the same point should be related.
One of the main advantages of this approach over more traditional optimization based approaches is the ability to add geometric consistency information into the loss at training time, even if it is not available at test time.
The simplest way to add geometric consistency losses, and the approach we use here, is to use the epipolar constraint between two cameras $i$ and $j$ on corresponding calibrated feature locations $X_{i,k}$ and $X_{j,l}$ is (derived in \cite{tron2014quotient}) shown in \ref{eq:epipolar}.
Given our vertex embeddings matrix $\mat{F}_V$, we can formulate a loss between all cameras:
\begin{align} 
\mathcal{L}_{geom}(\mat{F}_V)
=&\; \sum_{k,l} (\mat{F}_V)_{k} \cdot (\mat{F}_V)_{l} (\mat{G})_{kl} \label{eq:geom_cost2} \\
(\mat{G})_{kl} =&\; \left|X_{k}^\top R_{c(k)}^\top \cross{T_{c(l)} - T_{c(k)}}R_{c(l)} X_{l}\right| \label{eq:epipolar}
\end{align}
Where $c(k)$ is the appropriate camera for point index $k$.

\subsection{Rome 16K Graph Dataset}
We  use the Rome16K dataset \cite{li2010location} to test our algorithm in real world settings.
Rome16K consists of 16 thousand images of various historical sites in Rome extracted from Flickr, along with the 3D structure of the sites provided by bundle adjustment.
While not a standard dataset to test cycle consistency, other datasets had insufficient data to train a network on.
For these experiments we train with the $L_1$ norm and geometric consistency losses.
We evaluate on a test set using the ground truth adjacency matrix, which we compute from the bundle adjustment given by the Rome16K dataset.
We compare our method to spectral and optimization based baselines with different maximum iteration cutoffs.
Figure \ref{fig:errorlines} illustrates this by plotting the means of various metrics and their $25^{th}$ and $75^{th}$ percentiles.



\subsubsection*{Acknowledgements}
Support by ARL DCIST CRA W911NF-17-2-0181, NSF-IIS-1703319, ARL RCTA W911NF-10-2-0016, ONR N00014-17-1-2093, and the Honda Research Institute is gratefully acknowledged.

{\small
\bibliographystyle{ieee}
\bibliography{egbib}
}


\end{document}
