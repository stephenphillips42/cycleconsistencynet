\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{lowe2004distinctive}
\citation{fischler1981random}
\citation{wang2017multi}
\citation{yi2016lift}
\citation{li2010location}
\citation{li2010location}
\citation{kipf2016semi}
\citation{kipf2016semi}
\@writefile{toc}{\contentsline {section}{\numberline {1}\hskip -1em.\nobreakspace  {}Introduction}{1}{section.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces  Example of multi-image matching and cycle consistency, with images from Rome16K dataset \cite  {li2010location}. Pairwise matches between image can lead to global inconsistencies in the matching. In this example, an error in matching leads to an inconsistent cycle (shown in red), ideally these pairwise matches would be consistent. We improve the quality of pairwise matches in the multi-view matching problem by training a neural net with cycle consistency. }}{1}{figure.1}}
\newlabel{fig:cycconsistex}{{1}{1}{Example of multi-image matching and cycle consistency, with images from Rome16K dataset \cite {li2010location}. Pairwise matches between image can lead to global inconsistencies in the matching. In this example, an error in matching leads to an inconsistent cycle (shown in red), ideally these pairwise matches would be consistent. We improve the quality of pairwise matches in the multi-view matching problem by training a neural net with cycle consistency}{figure.1}{}}
\newlabel{fig:onecol}{{1}{1}{Example of multi-image matching and cycle consistency, with images from Rome16K dataset \cite {li2010location}. Pairwise matches between image can lead to global inconsistencies in the matching. In this example, an error in matching leads to an inconsistent cycle (shown in red), ideally these pairwise matches would be consistent. We improve the quality of pairwise matches in the multi-view matching problem by training a neural net with cycle consistency}{figure.1}{}}
\citation{li2010location}
\citation{fischler1981random}
\citation{lowe2004distinctive}
\citation{bay2006surf}
\citation{calonder2012brief}
\citation{mur2015orb}
\citation{suh2015subgraph}
\citation{hu2016distributable}
\citation{pachauri2013solving}
\citation{arrigoni2017synchronization}
\citation{zhou2015multi}
\citation{wang2017multi}
\citation{leonardos2016distributed}
\citation{tron2017fast}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces  An illustration of the approach of this work. The Graph Convolutional Neural Network (GCN) \cite  {kipf2016semi} takes as input the graph of matches and then outputs a low rank embedding of the adjacency matrix of the graph. The GCN operates on an embedding over the nodes of the graph. In the figure, the GCN node embeddings are represented by different colors. The final embedding is used to construct a pairwise similarity matrix, which we train to be a low dimensional cycle-consistent representation of the graph adjacency matrix, thus pruning the erroneous matches. We train the network using a reconstruction loss on the similarity matrix with the noisy adjacency matrix, and thus do not need ground truth matches. In addition, we can use geometric consistency information, such as epipolar constraints, to assist training the network. }}{2}{figure.2}}
\newlabel{fig:pipeline}{{2}{2}{An illustration of the approach of this work. The Graph Convolutional Neural Network (GCN) \cite {kipf2016semi} takes as input the graph of matches and then outputs a low rank embedding of the adjacency matrix of the graph. The GCN operates on an embedding over the nodes of the graph. In the figure, the GCN node embeddings are represented by different colors. The final embedding is used to construct a pairwise similarity matrix, which we train to be a low dimensional cycle-consistent representation of the graph adjacency matrix, thus pruning the erroneous matches. We train the network using a reconstruction loss on the similarity matrix with the noisy adjacency matrix, and thus do not need ground truth matches. In addition, we can use geometric consistency information, such as epipolar constraints, to assist training the network}{figure.2}{}}
\newlabel{fig:onecol}{{2}{2}{An illustration of the approach of this work. The Graph Convolutional Neural Network (GCN) \cite {kipf2016semi} takes as input the graph of matches and then outputs a low rank embedding of the adjacency matrix of the graph. The GCN operates on an embedding over the nodes of the graph. In the figure, the GCN node embeddings are represented by different colors. The final embedding is used to construct a pairwise similarity matrix, which we train to be a low dimensional cycle-consistent representation of the graph adjacency matrix, thus pruning the erroneous matches. We train the network using a reconstruction loss on the similarity matrix with the noisy adjacency matrix, and thus do not need ground truth matches. In addition, we can use geometric consistency information, such as epipolar constraints, to assist training the network}{figure.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}\hskip -1em.\nobreakspace  {}Related Work}{2}{section.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}\hskip -1em.\nobreakspace  {}Feature Matching}{2}{subsection.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}\hskip -1em.\nobreakspace  {}Multi-image Matching}{2}{subsection.2.2}}
\citation{zagoruyko2015learning}
\citation{yi2016lift}
\citation{brachmann2017dsac}
\citation{choy2016universal}
\citation{yi2018learning}
\citation{zhu2017unpaired}
\citation{bronstein2017geometric}
\citation{bruna2013spectral}
\citation{defferrard2016convolutional}
\citation{kipf2016semi}
\citation{scarselli2009graph}
\citation{gama2018mimo}
\citation{gama2018convolutional}
\citation{battaglia2018relational}
\citation{bruna2013spectral}
\citation{bronstein2017geometric}
\citation{kipf2016semi}
\citation{scarselli2009graph}
\citation{gama2018convolutional}
\citation{gama2018mimo}
\citation{gama2018convolutional}
\citation{battaglia2018relational}
\citation{suh2015subgraph}
\citation{yi2018learning}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}\hskip -1em.\nobreakspace  {}Deep Learning for Matching}{3}{subsection.2.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}\hskip -1em.\nobreakspace  {}Graph Neural Networks}{3}{subsection.2.4}}
\@writefile{toc}{\contentsline {section}{\numberline {3}\hskip -1em.\nobreakspace  {}Method}{3}{section.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces  Illustration of the universe of features. Each feature in each image corresponds to a 3D point in the scene. We can construct cycle consistent embeddings of the features by mapping each one to the one-hot vector of its corresponding 3D point. While there can be many features, there are fewer 3D points and thus this corresponds to a low rank factorization of the correspondence matrix. Best viewed in color. }}{3}{figure.3}}
\newlabel{fig:universefeatures}{{3}{3}{Illustration of the universe of features. Each feature in each image corresponds to a 3D point in the scene. We can construct cycle consistent embeddings of the features by mapping each one to the one-hot vector of its corresponding 3D point. While there can be many features, there are fewer 3D points and thus this corresponds to a low rank factorization of the correspondence matrix. Best viewed in color}{figure.3}{}}
\newlabel{fig:onecol}{{3}{3}{Illustration of the universe of features. Each feature in each image corresponds to a 3D point in the scene. We can construct cycle consistent embeddings of the features by mapping each one to the one-hot vector of its corresponding 3D point. While there can be many features, there are fewer 3D points and thus this corresponds to a low rank factorization of the correspondence matrix. Best viewed in color}{figure.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}\hskip -1em.\nobreakspace  {}Correspondence Graph}{3}{subsection.3.1}}
\citation{bruna2013spectral}
\citation{bronstein2017geometric}
\citation{gama2018mimo}
\citation{kipf2016semi}
\citation{gama2018convolutional}
\citation{battaglia2018relational}
\citation{wu2018group}
\citation{zhou2015multi}
\citation{wang2017multi}
\citation{leonardos2016distributed}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}\hskip -1em.\nobreakspace  {}Graph Neural Networks}{4}{subsection.3.2}}
\newlabel{eq:graph_conv}{{4}{4}{\hskip -1em.~Graph Neural Networks}{equation.3.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces  Illustrated here is an example of how the geometric loss is computed for one feature. Errors are computed via absolute distance from the epipolar line, as expressed by (\ref  {eq:essential_constraint}) via the epipolar constraint. The epipolar line is the line of projection of the feature on the first image, projected onto to the second. The distance to this line on the second image indicates how likely that point is to correspond geometrically to the original feature. There can be false positives along the projected line, as shown by the square feature in the figure, but other points will be eliminated, such as the hexagonal feature. Best viewed in color. }}{4}{figure.4}}
\newlabel{fig:geoconsist}{{4}{4}{Illustrated here is an example of how the geometric loss is computed for one feature. Errors are computed via absolute distance from the epipolar line, as expressed by (\ref {eq:essential_constraint}) via the epipolar constraint. The epipolar line is the line of projection of the feature on the first image, projected onto to the second. The distance to this line on the second image indicates how likely that point is to correspond geometrically to the original feature. There can be false positives along the projected line, as shown by the square feature in the figure, but other points will be eliminated, such as the hexagonal feature. Best viewed in color}{figure.4}{}}
\newlabel{fig:onecol}{{4}{4}{Illustrated here is an example of how the geometric loss is computed for one feature. Errors are computed via absolute distance from the epipolar line, as expressed by (\ref {eq:essential_constraint}) via the epipolar constraint. The epipolar line is the line of projection of the feature on the first image, projected onto to the second. The distance to this line on the second image indicates how likely that point is to correspond geometrically to the original feature. There can be false positives along the projected line, as shown by the square feature in the figure, but other points will be eliminated, such as the hexagonal feature. Best viewed in color}{figure.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}\hskip -1em.\nobreakspace  {}Cycle Consistency}{4}{subsection.3.3}}
\newlabel{eq:cycconsist1}{{5}{4}{\hskip -1em.~Cycle Consistency}{equation.3.5}{}}
\newlabel{eq:cycconsist2}{{6}{4}{\hskip -1em.~Cycle Consistency}{equation.3.6}{}}
\citation{tron2014quotient}
\citation{zhou2015multi}
\citation{zhou2015multi}
\citation{zhou2015multi}
\citation{leonardos2016distributed}
\citation{leonardos2016distributed}
\citation{leonardos2016distributed}
\citation{zhou2015multi}
\citation{zhou2015multi}
\citation{zhou2015multi}
\citation{leonardos2016distributed}
\citation{leonardos2016distributed}
\citation{leonardos2016distributed}
\citation{zhou2015multi}
\citation{zhou2015multi}
\citation{leonardos2016distributed}
\citation{zhou2015multi}
\citation{zhou2015multi}
\citation{leonardos2016distributed}
\citation{kingma2014adam}
\citation{li2010location}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces  Results on Synthetic correspondence graphs. The `Same Point Similarities' column is the mean and standard deviation of similarities for true corresponding points, while the `Different Point Similarities' is the same for points that do not correspond. For the `Same Point Similarities' column higher is better, and for `Different Point Similarities' lower is better. Losses tested against ground truth correspondence graph adjacency matrices. Our method was not trained on ground truth correspondences but using unsupervised methods. }}{5}{table.1}}
\newlabel{fig:synthtable}{{1}{5}{Results on Synthetic correspondence graphs. The `Same Point Similarities' column is the mean and standard deviation of similarities for true corresponding points, while the `Different Point Similarities' is the same for points that do not correspond. For the `Same Point Similarities' column higher is better, and for `Different Point Similarities' lower is better. Losses tested against ground truth correspondence graph adjacency matrices. Our method was not trained on ground truth correspondences but using unsupervised methods}{table.1}{}}
\newlabel{eq:rotinvar}{{8}{5}{\hskip -1em.~Cycle Consistency}{equation.3.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}\hskip -1em.\nobreakspace  {}Geometric Consistency Loss}{5}{subsection.3.4}}
\newlabel{eq:essential_constraint_rel}{{9}{5}{\hskip -1em.~Geometric Consistency Loss}{equation.3.9}{}}
\newlabel{eq:essential_constraint}{{10}{5}{\hskip -1em.~Geometric Consistency Loss}{equation.3.10}{}}
\newlabel{eq:geom_cost}{{11}{5}{\hskip -1em.~Geometric Consistency Loss}{equation.3.11}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}\hskip -1em.\nobreakspace  {}Experiments}{5}{section.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}\hskip -1em.\nobreakspace  {}Synthetic Graph Dataset}{5}{subsection.4.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}\hskip -1em.\nobreakspace  {}Rome 16K Graph Dataset}{5}{subsection.4.2}}
\citation{kingma2014adam}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces  Results on Rome16K Correspondence graphs, showing the mean and standard deviation of the $L_1$ and $L_2$. Our method was not trained on ground truth correspondences but using unsupervised methods and geometric side losses. As our method gives soft labels, we use cannot use precision or recall as is standard in testing cycle consistency \cite  {zhou2015multi}. Thus we test against ground truth correspondence graph adjacency matrices computed from the bundle adjustment output. Our method performs better than 25 iteration of the MatchALS \cite  {zhou2015multi} method, but does not perform as well as 50 iterations. We do not perform as well as the Projected Gradient Descent Doubly Stochastic (PGDDS) \cite  {leonardos2016distributed} but we perform significantly faster than them. Note that we perform much better in $L_1$ performance rather than $L_2$, as we optimized the network weights using an $L_1$ loss. }}{6}{table.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces  Training curves with and without Geometric Training loss. The geometric training loss improves testing performance. Shown as horizontal lines are the state of the art optimization based baselines. Even with a simple network we compare well with them. }}{6}{figure.5}}
\newlabel{fig:short}{{5}{6}{Training curves with and without Geometric Training loss. The geometric training loss improves testing performance. Shown as horizontal lines are the state of the art optimization based baselines. Even with a simple network we compare well with them}{figure.5}{}}
\citation{zhou2015multi}
\citation{leonardos2016distributed}
\citation{bremner1994hinton}
\citation{bremner1994hinton}
\bibstyle{ieee}
\bibdata{mybib}
\bibcite{arrigoni2017synchronization}{1}
\bibcite{battaglia2018relational}{2}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Example output of our network. (a) Similarity Matrix of the of the embeddings (b) Histogram of feature similarities for pairs which correspond to the same 3D point, comparing our output features with the original SIFT features (c) Histogram of feature similarities for pairs which correspond to different 3D points, comparing our output features with the original SIFT features}}{7}{figure.6}}
\newlabel{fig:short}{{6}{7}{Example output of our network. (a) Similarity Matrix of the of the embeddings (b) Histogram of feature similarities for pairs which correspond to the same 3D point, comparing our output features with the original SIFT features (c) Histogram of feature similarities for pairs which correspond to different 3D points, comparing our output features with the original SIFT features}{figure.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}\hskip -1em.\nobreakspace  {}Conclusion}{7}{section.5}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces  Visualization of the learned embeddings using Hinton diagrams \cite  {bremner1994hinton}. On the left we have the raw outputs, which are difficult to interpret. On the right, we rotated the features to best match the ground truth for a more interpretable visualization. }}{7}{figure.7}}
\newlabel{fig:onecol}{{7}{7}{Visualization of the learned embeddings using Hinton diagrams \cite {bremner1994hinton}. On the left we have the raw outputs, which are difficult to interpret. On the right, we rotated the features to best match the ground truth for a more interpretable visualization}{figure.7}{}}
\newlabel{fig:embeddingsviz}{{7}{7}{Visualization of the learned embeddings using Hinton diagrams \cite {bremner1994hinton}. On the left we have the raw outputs, which are difficult to interpret. On the right, we rotated the features to best match the ground truth for a more interpretable visualization}{figure.7}{}}
\bibcite{bay2006surf}{3}
\bibcite{brachmann2017dsac}{4}
\bibcite{bremner1994hinton}{5}
\bibcite{bronstein2017geometric}{6}
\bibcite{bruna2013spectral}{7}
\bibcite{calonder2012brief}{8}
\bibcite{choy2016universal}{9}
\bibcite{defferrard2016convolutional}{10}
\bibcite{fischler1981random}{11}
\bibcite{gama2018convolutional}{12}
\bibcite{gama2018mimo}{13}
\bibcite{hu2016distributable}{14}
\bibcite{kingma2014adam}{15}
\bibcite{kipf2016semi}{16}
\bibcite{leonardos2016distributed}{17}
\bibcite{li2010location}{18}
\bibcite{lowe2004distinctive}{19}
\bibcite{mur2015orb}{20}
\bibcite{pachauri2013solving}{21}
\bibcite{scarselli2009graph}{22}
\bibcite{suh2015subgraph}{23}
\bibcite{tron2014quotient}{24}
\bibcite{tron2017fast}{25}
\bibcite{wang2017multi}{26}
\bibcite{wu2018group}{27}
\bibcite{yi2016lift}{28}
\bibcite{yi2018learning}{29}
\bibcite{zagoruyko2015learning}{30}
\bibcite{zhou2015multi}{31}
\bibcite{zhu2017unpaired}{32}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces  This figure plots the average accuracy of the optimization methods by iteration compared to 12 iterations of our GCN based method. The bold center line shows the mean, the colored in region shows 1 standard devation of the errors. As we optimize for $L_1$ error, our method's $L_1$ loss is substantially better than its $L_2$ losses. The right most plot shows time, plotted on a log scale for clarity. }}{9}{figure.8}}
\newlabel{fig:iterplot}{{8}{9}{This figure plots the average accuracy of the optimization methods by iteration compared to 12 iterations of our GCN based method. The bold center line shows the mean, the colored in region shows 1 standard devation of the errors. As we optimize for $L_1$ error, our method's $L_1$ loss is substantially better than its $L_2$ losses. The right most plot shows time, plotted on a log scale for clarity}{figure.8}{}}
\newlabel{fig:onecol}{{8}{9}{This figure plots the average accuracy of the optimization methods by iteration compared to 12 iterations of our GCN based method. The bold center line shows the mean, the colored in region shows 1 standard devation of the errors. As we optimize for $L_1$ error, our method's $L_1$ loss is substantially better than its $L_2$ losses. The right most plot shows time, plotted on a log scale for clarity}{figure.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces  This figure plots the training curves of the network with and without Group Normalization. Note that while the train/test curves are fairly similar with or without Group Normalization, distance to the ground truth is improved greatly with Group Normalization. }}{9}{figure.9}}
\newlabel{fig:groupnorm}{{9}{9}{This figure plots the training curves of the network with and without Group Normalization. Note that while the train/test curves are fairly similar with or without Group Normalization, distance to the ground truth is improved greatly with Group Normalization}{figure.9}{}}
\newlabel{fig:onecol}{{9}{9}{This figure plots the training curves of the network with and without Group Normalization. Note that while the train/test curves are fairly similar with or without Group Normalization, distance to the ground truth is improved greatly with Group Normalization}{figure.9}{}}
